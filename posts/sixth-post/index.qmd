---
title: "Webscraping"
description: "Functions from Lab 8"
author:
  - name: Jose Garcia
date: 2025-05-26
categories: [R]
draft: false
format:
  html:
    page-layout: full
    embed-resources: true
editor: source
---

> **Goal:** Scrape information from <https://www.cheese.com> to obtain a dataset of characteristics about different cheeses, and gain deeper insight into your coding process. ðŸª¤

**Part 1:** Locate and examine the `robots.txt` file for this website. Summarize what you learn from it.

-   The only lines on 'https://www.cheese.com/robots.txt' is User-agent: \* and Sitemap: https://www.cheese.com/sitemap.xml .

-   'User-agent: \*' informs us that anyone is allowed to scrape.

-   The lack of Crawl-delay, Visit-time and Request-rate means that we can scrape as often as we want, at any time of day, from as many different users as we want.

-   The lack of a 'Disallow" section means that there are no scraping restrictions on specific areas of the cheese website.

**Part 2:** Learn about the `html_attr()` function from `rvest`. Describe how this function works with a small example.

```{r}
library(rvest)
```

-   The html_attr() function obtains the value of an attribute. It is frequently used after html_elements on a website.

-   Below, html_elements("a") extracts all elements inside the <a> attribute. html_attr("href") extracts the value of href inside the <a> element. In this case the href is the link to the wikipedia article on cats.

-   Example below can be found [here](https://cran.r-project.org/web/packages/rvest/vignettes/rvest.html).

```{r}
html <- minimal_html("
  <p><a href='https://en.wikipedia.org/wiki/Cat'>cats</a></p>
  <img src='https://cataas.com/cat' width='100' height='200'>
")

html %>% 
  html_elements("a") %>% 
  html_attr("href")
```

**Part 3:** (Do this alongside Part 4 below.) I used [ChatGPT](https://chat.openai.com/chat) to start the process of scraping cheese information with the following prompt:

> Write R code using the rvest package that allows me to scrape cheese information from cheese.com.

Fully document your process of checking this code. Record any observations you make about where ChatGPT is useful / not useful.

```{r}
#| eval: false
#| label: small-example-of-getting-cheese-info_unedited

# Load required libraries
library(rvest)
library(dplyr)

# Define the URL
url <- "https://www.cheese.com/alphabetical"

# Read the HTML content from the webpage
webpage <- read_html(url)

# Extract the cheese names and URLs
cheese_data <- webpage %>%
  html_nodes(".cheese-item") %>%
  html_nodes("a") %>%
  html_attr("href") %>%
  paste0("https://cheese.com", .)

cheese_names <- webpage %>%
  html_nodes(".cheese-item h3") %>%
  html_text()

# Create a data frame to store the results
cheese_df <- data.frame(Name = cheese_names,
                        URL = cheese_data,
                        stringsAsFactors = FALSE)

# Print the data frame
print(cheese_df)
```

-   The url found the list of cheeses on the website and read it in using read_html, which is a good start.

-   Everything in the code chunk below works well.

```{r}
#| eval: false


# Load required libraries
library(rvest)
library(dplyr)

# Define the URL
url <- "https://www.cheese.com/alphabetical"

# Read the HTML content from the webpage
webpage <- read_html(url)
```

-   cheese_data is attempting to check all cheese items and extract their urls. However, the html_nodes(".cheese-item") is not correctly finding all cheeses, which makes the rest of the function produce only "https://cheese.com" without specific cheeses appended at the end of the url.

-   Running ' webpage %\>% html_nodes(".cheese-item")' produces a warning saying it could not find the child attributes.

-   cheese_names is supposed to be the extracting names of the cheese from each url. Since cheese_data is not a list of cheese url's like it is supposed to be, cheese_names does not contain a list of cheese names.

-   The cheese_df object fails because cheese_data contiains one url, and cheese_names is empty since it didn't find any cheeses.

```{r}
#| eval: false

# Extract the cheese names and URLs
cheese_data <- webpage %>%
  html_nodes(".cheese-item") %>%
  html_nodes("a") %>%
  html_attr("href") %>%
  paste0("https://cheese.com", .)

cheese_names <- webpage %>%
  html_nodes(".cheese-item h3") %>%
  html_text()

# Create a data frame to store the results
cheese_df <- data.frame(Name = cheese_names,
                        URL = cheese_data,
                        stringsAsFactors = FALSE)

# Print the data frame
print(cheese_df)
```

**Part 4:** Obtain the following information for **all** cheeses in the database:

-   cheese name
-   URL for the cheese's webpage (e.g., <https://www.cheese.com/gouda/>)
-   whether or not the cheese has a picture (e.g., [gouda](https://www.cheese.com/gouda/) has a picture, but [bianco](https://www.cheese.com/bianco/) does not).

To be kind to the website owners, please add a 1 second pause between page queries. (Note that you can view 100 cheeses at a time.)

```{r}
library(stringr)
library(rvest)

scrape_cheese <- function(total_pages) {
  
  cheese_data <- c()
  cheese_names <- c()
  cheese_img <- c()
  all_urls <- c()
  
  # loop through each page
  for (i in 1:total_pages) {
    
    # read each url
    url <- paste0("https://www.cheese.com/alphabetical/?per_page=100&page=", i)
    Sys.sleep(1)
    webpage <- read_html(url)
    
    # scrape webpage for cheese data
    data <- webpage |>
      html_elements("h3 a") |>
      html_attr("href") 
    
    # create and append each url
    url <- paste0("https://cheese.com", data)
    all_urls <- c(all_urls, url)
    
    # scrape for cheese names
    names <- webpage |>
      html_nodes("h3 a") |>
      html_text()
    
    # scrape for missing image identifier
    img <- webpage |>
      html_nodes(".product-item img") |>
      html_attr("class")
    
    # if image-missing, return TRUE
    missing_img <- !str_detect(img, "image-missing")
    
    # append data
    cheese_data <- c(cheese_data, data)
    cheese_names <- c(cheese_names, names)
    cheese_img <- c(cheese_img, missing_img)
      
  }
  
  # create data frame
  cheese_df <- data.frame(
    Name = cheese_names,
    URL = all_urls,
    Has_Picture = cheese_img,
    stringsAsFactors = FALSE
  )
  
  # reorder df by name
  cheese_df <- cheese_df[order(cheese_df$Name), ]
  
  return(cheese_df)
  
}

df <-scrape_cheese(5)

names

```

**Part 5:** When you go to a particular cheese's page (like [gouda](https://www.cheese.com/gouda/)), you'll see more detailed information about the cheese. For [**just 10**]{.underline} of the cheeses in the database, obtain the following detailed information:

-   milk information
-   country of origin
-   family
-   type
-   flavour

(Just 10 to avoid overtaxing the website! Continue adding a 1 second pause between page queries.)

```{r}
library(stringr)
library(dplyr)
library(tidyverse)
```

```{r}
#| label: cheese_info_function

get_cheese_info = function(cheese){
  #waiting one second between API calls
  Sys.sleep(1)
  
  #taking provided url and reading in it's html elements
  url = cheese
  webpage = read_html(url)
  
  #extracting name of cheese
  name = webpage %>% html_elements("h1") %>% html_text2()
  
  #extracting href from milk element
  milk = webpage %>% html_elements(".summary_milk p") %>% html_elements('a') %>% 
  html_attr("href") %>% str_split_i("/by_milk/", 2) %>% str_remove("/")
  
  #extracting href from country element
  country = webpage %>% html_elements(".summary_country p") %>% html_elements('a') %>% 
  html_attr("href") %>% str_split_i("/by_country/", 2) %>% str_remove("/")
  
  #extracting desired text element from family element
  #family typically doesn't have an href
  family = webpage %>% html_elements(".summary_family p") %>% html_text2() %>% 
  str_split_i("Family: ", 2)
  
  #extracting href from type element
  type = webpage %>% html_elements(".summary_moisture_and_type p") %>% html_elements('a') %>%
  html_attr("href") %>% str_split_i("/by_type/", 2) %>% str_remove("/")
  
  #extracting desired text element from flavour element
  #flavour typically doesn't have an href
  flavour = webpage %>% html_elements(".summary_taste p") %>% html_text2() %>%
  str_split_i("Flavour: ", 2) %>% str_remove("/")
  
  
  #combining 6 extracted elements into a data frame.
  #paste0(x, collapse=",") puts cheeses with multiple of one response (eg. milk)
  #into one cell
  df = data.frame(name = paste0(name, collapse = ", "), 
                    milk = paste0(milk, collapse = ", "), 
                    country = paste0(country, collapse = ", "), 
                    family = paste0(family, collapse = ", "), 
                    type = paste0(type, collapse = ", "), 
                    flavour = paste0(flavour, collapse = ", ")) %>% 
  mutate(across(c(name, milk, country, family, type, flavour), na_if, ""))
  #mutate replaces "" with NA values
  
  return(df)
}
```

```{r}
#| label: 10_cheeses_test

#list of 10 cheeses to extract attributes
cheese_list = c("https://www.cheese.com/gouda/", 
                "https://www.cheese.com/colby-jack/",
                "https://www.cheese.com/swiss/",
                "https://www.cheese.com/cheddar/",
                "https://www.cheese.com/tetilla/",
                "https://www.cheese.com/aura/",
                "https://www.cheese.com/butterkase/",
                "https://www.cheese.com/vasterbottenost/",
                "https://www.cheese.com/muenster/",
                "https://www.cheese.com/adelost/")


#using map to call the above function on each cheese in the list
#bind_rows combines all 10 returned data into one 10 row data frame
cheese_info = bind_rows(map(cheese_list, get_cheese_info))

```

**Part 6:** Evaluate the code that you wrote in terms of **efficiency**. To what extent do your function(s) adhere to the **principles for writing good functions**? To what extent are your **functions efficient**? To what extent is your **iteration of these functions efficient**?

Efficiency: To increase efficiency in our scrape_cheese function we initiated vectors to hold the elements of our final data set. We knew we would need these eventually so allocating vectors to them vectorizes our function. We also used function like across() in our get_cheese_info function to avoid writing a more complex function and ensuring that we get the same result each time the function runs.

Limitations in Iteration: We did use a for loop in our scrape_cheese function to iterate through pages on the cheese website. For loops are not optimally efficient in R so reformatting the code to use a map function or apply function could further increase our efficiency.

Principles of Writing Good Functions: We used the data.frame function in both functions that we wrote to ensure that output is of the same form each time the function runs. We also gave each element in our functions practical yet descriptive names that should tell anyone familiar with R what is happening in the body of our code and what the end goal should contain. Our functions are also self contained not relying on any information from outside the function.
